{"cells":[{"cell_type":"code","execution_count":14,"id":"de649b96","metadata":{"id":"de649b96","executionInfo":{"status":"ok","timestamp":1646205591559,"user_tz":360,"elapsed":144,"user":{"displayName":"Dean Todd","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcGJyalaa3B-ZuPUjRl03nXuCEJsJ00R4UGTQ-4g=s64","userId":"15521228335461892819"}}},"outputs":[],"source":["import numpy as np\n","from sympy import Matrix\n","from sympy import symbols\n","from sympy import lambdify\n","from sympy.solvers import solve"]},{"cell_type":"markdown","source":["## Task 1: Golden ratio line search"],"metadata":{"id":"uKCAiNxPUfm4"},"id":"uKCAiNxPUfm4"},{"cell_type":"code","execution_count":15,"id":"47dce12d","metadata":{"id":"47dce12d","executionInfo":{"status":"ok","timestamp":1646205596127,"user_tz":360,"elapsed":126,"user":{"displayName":"Dean Todd","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcGJyalaa3B-ZuPUjRl03nXuCEJsJ00R4UGTQ-4g=s64","userId":"15521228335461892819"}}},"outputs":[],"source":["def gsSearch(f,x=0,d=0,eps=10e-5):\n","  ''' perform golden ratio search\n","      input:\n","        f - the function to be evaluated\n","        x - current iteration optimization variable\n","        d - direction to search\n","        eps - small number that serves to terminate the search\n","      output:\n","        alpha - parameter to multiply by d to calculate the next x\n","  '''\n","  \n","#################################\n","# Start entering your code here #\n","#################################\n","\n","  while d >= eps:\n","    pass\n","  alpha = 1\n","\n","#################################\n","# End of your code              #\n","#################################\n","\n","  return alpha"]},{"cell_type":"markdown","source":["## Task 2: Steepest descent"],"metadata":{"id":"n87x2E3XYYzB"},"id":"n87x2E3XYYzB"},{"cell_type":"code","source":["def sd(f, x=0, der=0, eps=10e-5):\n","  ''' Steepest descent\n","      input:\n","        f - objective function as a python function\n","        x - initial guess of the optimal solution\n","        der - analytical derivative of the objective function as a python function\n","        eps - small number to terminate the algorithm\n","      output:\n","        x_out - solution of x from all the iteration (not just the optimal value)\n","  '''\n","\n","  \n","#################################\n","# Start entering your code here #\n","#################################\n","\n","\n","\n","#################################\n","# End of your code              #\n","#################################\n","\n","  return x_out"],"metadata":{"id":"rGlOSRbOLdHD"},"id":"rGlOSRbOLdHD","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 3: Conjugate Gradient"],"metadata":{"id":"G5bwtfE8IoSE"},"id":"G5bwtfE8IoSE"},{"cell_type":"code","source":["def cg(f, x=0, der=0, eps=10e-5):\n","  ''' Conjugate gradient\n","      input:\n","        f - objective function as a python function\n","        x - initial guess of the optimal solution\n","        der - analytical derivative of the objective function as a python function\n","        eps - small number to terminate the algorithm\n","      output:\n","        x_out - solution of x from all the iteration (not just the optimal value)\n","  '''\n","\n","#################################\n","# Start entering your code here #\n","#################################\n","\n","  i = 0\n","  xi = x\n","  xi1 = x\n","  exes = [xi,xi1]\n","  c0 = der(exes)\n","  d0 = -1*c0\n","  if abs(d) <= eps:\n","    a = gsSearch(f,x,d0,eps)\n","    x_out = x + a*d0\n","  else:\n","    ci = c0\n","    while abs(ci) >= eps:\n","      cim1 = ci\n","      dim1 = di\n","      ci = der(exes)\n","      bi = (ci.T * ci) / (cim1.T * cim1)\n","      di = -1*ci + bi*dim1\n","      a = gsSearch(f,xi,di,eps)\n","      xi1 = xi + a*di\n","      xi = xi1\n","      exes = [xi,xi1]\n","      i += 1\n","      if i > 300000:\n","        break\n","    x_out = xi1\n","  \n","#################################\n","# End of your code              #\n","#################################\n","\n","  return x_out"],"metadata":{"id":"hm8KvQ44Inh4","executionInfo":{"status":"ok","timestamp":1646205598919,"user_tz":360,"elapsed":127,"user":{"displayName":"Dean Todd","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcGJyalaa3B-ZuPUjRl03nXuCEJsJ00R4UGTQ-4g=s64","userId":"15521228335461892819"}}},"id":"hm8KvQ44Inh4","execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Task 4: Modified Newton Method"],"metadata":{"id":"JgiqVbiXNZVk"},"id":"JgiqVbiXNZVk"},{"cell_type":"code","source":["def mn(f, x=0, der=0, hes=0, eps=10e-5):\n","  ''' Modified Newton Method\n","      input:\n","        f - objective function as a python function\n","        x - initial guess of the optimal solution\n","        der - analytical derivative of the objective function as a python function\n","        hes - analytical hessian of the objective function as a python function\n","        eps - small number to terminate the algorithm\n","      output:\n","        x_out - solution of x from all the iteration (not just the optimal value)\n","  '''\n","\n","#################################\n","# Start entering your code here #\n","#################################\n","\n","  \n","#################################\n","# End of your code              #\n","#################################\n","\n","  return x_out"],"metadata":{"id":"jDTMxKiMNT9l"},"id":"jDTMxKiMNT9l","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test case 1: Quadratic function"],"metadata":{"id":"U7jJ9lb2Mkqn"},"id":"U7jJ9lb2Mkqn"},{"cell_type":"code","source":["#######################################\n","# Test case 1\n","#######################################\n","\n","# Objective function\n","def testFun(x):\n","  return x[0]**2+x[1]**2\n","\n","# Derivative of the objective function\n","def testFunDer(x):\n","  return np.array([[2*x[0]], [2*x[1]]])\n","\n","\n","# Derivative of the objective function\n","def testFunHes(x):\n","  return np.array([[2, 0], [0, 2]])\n","\n","# execution of your optimization code #\n","x0 = np.array([-25., 75.])\n","\n","x_out_sd = sd(testFun,x0,testFunDer,0.00001)\n","print('Steepest descent gives: ' + str(x_out_sd[-1]))\n","\n","x_out_cg = cg(testFun,x0,testFunDer,0.00001)\n","print('Conjugate gradient gives: ' + str(x_out_cg[-1]))\n","\n","x_out_mn = mn(testFun,x0,testFunDer,testFunHes, 0.00001)\n","print('Modified Newton gives: ' + str(x_out_mn[-1]))\n","\n","# Check your code against scipy optimize\n","from scipy.optimize import minimize\n","res = minimize(testFun, x0)\n","print('Scipy gives: ' + str(res.x))\n"],"metadata":{"id":"IpUwi8TdW_Sx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646200881636,"user_tz":360,"elapsed":184,"user":{"displayName":"Dean Todd","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcGJyalaa3B-ZuPUjRl03nXuCEJsJ00R4UGTQ-4g=s64","userId":"15521228335461892819"}},"outputId":"2f93d787-6524-4434-8cc1-a573614fc77e"},"id":"IpUwi8TdW_Sx","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0]\n"," [0]]\n"]}]},{"cell_type":"code","source":["#######################################\n","# Test case 1\n","#######################################\n","\n","# Plot the contour and the optimization iteration\n","import matplotlib.pyplot as plt\n","\n","n = 1000 # calculate for n data points \n","val_x = np.linspace(-100.,100., n)  \n","\n","# meshgrid allows us to weave these two variables to arrive at n^2 points for calculation\n","optVar_x0, optVar_x1 = np.meshgrid(val_x, val_x)\n","\n","# Plot the contour map\n","plt.contour(optVar_x0, optVar_x1, testFun([optVar_x0, optVar_x1]))\n","# Scatter plot of the iteration of x\n","plt.scatter(x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.plot   (x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.scatter(x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.plot   (x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.scatter(x_out_mn[:,0],x_out_mn[:,1], c='g')\n","plt.plot   (x_out_mn[:,0],x_out_mn[:,1], c='g')\n","\n","plt.show()"],"metadata":{"id":"RYL45M6bXTr0"},"id":"RYL45M6bXTr0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test case 2: Box optimization"],"metadata":{"id":"FkK4h358MncZ"},"id":"FkK4h358MncZ"},{"cell_type":"code","source":["#######################################\n","# Test case 2\n","#######################################\n","\n","v_req = 10**6\n","\n","from sympy import symbols\n","from sympy import lambdify\n","from sympy import Matrix\n","\n","# Define optimization variables\n","a, b = symbols('a b')\n","\n","# Define the objective to minimize\n","box  = a*b+2*v_req/b+2*v_req/a \n","dBox = Matrix([box.diff(x) for x in [a,b]]) # Calculate gradient\n","ddBox = Matrix([[y.diff(x) for y in dBox] for x in [a,b]]) # Calculate hessian\n","\n","# lambdify the objective function\n","boxF = lambdify([a,b], box)\n","dBoxF = lambdify([a,b], dBox)\n","ddBoxF = lambdify([a,b], ddBox)\n","\n","# Wrapper functions\n","def boxFW (x):\n","  return boxF(x[0],x[1])\n","\n","def dBoxFW (x):\n","  return dBoxF(x[0],x[1])\n","\n","def ddBoxFW (x):\n","  return ddBoxF(x[0],x[1])\n","\n","# execution of your optimization code #\n","x0 = np.array([60., 200.])\n","\n","x_out_sd = sd(boxFW,x0,dBoxFW,0.00001)\n","print('Steepest descent gives: ' + str(x_out_sd[-1]))\n","\n","x_out_cg = cg(boxFW,x0,dBoxFW,0.00001)\n","print('Conjugate gradient gives: ' + str(x_out_cg[-1]))\n","\n","x_out_mn = mn(boxFW,x0,dBoxFW,ddBoxFW,0.00001)\n","print('Modified Newton gives: ' + str(x_out_mn[-1]))\n","\n","# Check your code against scipy optimize\n","from scipy.optimize import minimize\n","res = minimize(boxFW, x0)\n","print('Scipy gives: ' + str(res.x))\n"],"metadata":{"id":"dSsRhz1i_doh"},"id":"dSsRhz1i_doh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################################\n","# Test case 2\n","#######################################\n","\n","n = 1000 # calculate for n data points \n","val_x = np.linspace(10., 220., 100)  \n","\n","# meshgrid allows us to weave these two variables to arrive at n^2 points for calculation\n","optVar_x0, optVar_x1 = np.meshgrid(val_x, val_x)\n","\n","import matplotlib.pyplot as plt\n","# initialize a figure container\n","fig = plt.figure(figsize=(8, 6))\n","\n","# Plot the contour map to show objective landscape\n","plt.contour(optVar_x0, optVar_x1, boxFW([optVar_x0, optVar_x1]))\n","# Scatter plot of the iteration of x_star values\n","\n","plt.scatter(x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.plot   (x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.scatter(x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.plot   (x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.scatter(x_out_mn[:,0],x_out_mn[:,1], c='g')\n","plt.plot   (x_out_mn[:,0],x_out_mn[:,1], c='g')\n","\n","plt.xlim([50,150])\n","plt.ylim([100,225])\n","plt.show()"],"metadata":{"id":"8AjtlVW_HPEh"},"id":"8AjtlVW_HPEh","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test case 3: Wire optimization"],"metadata":{"id":"2aF1k6c8Mx2h"},"id":"2aF1k6c8Mx2h"},{"cell_type":"code","source":["#######################################\n","# Test case 3\n","#######################################\n","\n","# Set all the parameters\n","y_start = 50; x_start = 50; x_end = 50; y_end = 350; a = 500; b = 400; t = 75\n","\n","from sympy import symbols\n","from sympy import lambdify\n","from sympy import Matrix\n","# Define optimization variables\n","y1, y2 = symbols('y1 y2')\n","\n","# Define the objective to minimize\n","length  = ((x_start-a)**2+(y_start-y1)**2)**0.5 + \\\n","          ((y1-y2)**2+t**2)**0.5 + \\\n","          ((a-x_end)**2+(y2-y_end)**2)**0.5\n","\n","# Calculate gradient then cast to sympy matrix\n","dLength = Matrix([length.diff(x) for x in [y1,y2]]) \n","ddLength = Matrix([[y.diff(x) for y in dLength] for x in [y1,y2]]) # Calculate hessian\n","\n","\n","# lambdify the objective function and the derivative\n","lengthF     = lambdify([y1,y2], length)\n","dLengthF     = lambdify([y1,y2], dLength)\n","ddLengthF     = lambdify([y1,y2], ddLength)\n","\n","# define wrapper functions\n","def lengthFW (x):\n","  return lengthF(x[0],x[1])\n","\n","def dLengthFW (x):\n","  return dLengthF(x[0],x[1])\n","\n","def ddLengthFW (x):\n","  return ddLengthF(x[0],x[1])\n","\n","\n","# execution of your optimization code #\n","x0 = np.array([20., 200.])\n","x_out_sd = sd(lengthFW,x0,dLengthFW,0.00001)\n","print('Steepest descent gives: ' + str(x_out_sd[-1]))\n","\n","x_out_cg = cg(lengthFW,x0,dLengthFW,0.00001)\n","print('Conjugate gradient gives: ' + str(x_out_cg[-1]))\n","\n","x_out_mn = mn(lengthFW,x0,dLengthFW,ddLengthFW,0.00001)\n","print('Modified Newton gives: ' + str(x_out_mn[-1]))\n","\n","# Check your code against scipy minimize\n","from scipy.optimize import minimize\n","res = minimize(lengthFW, x0)\n","print('Scipy gives: ' + str(res.x))"],"metadata":{"id":"J0ZjPll-LwiV"},"id":"J0ZjPll-LwiV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################################\n","# Test case 3\n","#######################################\n","\n","import matplotlib.pyplot as plt\n","\n","# calculate for n data points \n","n = 1000 \n","val_x = np.linspace(0., b, n)  \n","\n","# meshgrid allows us to weave these two variables to arrive at n^2 points for calculation\n","optVar_x0, optVar_x1 = np.meshgrid(val_x, val_x)\n","\n","# initialize a figure container\n","fig = plt.figure(figsize=(8, 6))\n","\n","# plot contour to display objective landscape\n","plt.contour(optVar_x0, optVar_x1, lengthFW([optVar_x0, optVar_x1]))\n","\n","# plot the iteration x_star values\n","\n","plt.scatter(x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.plot   (x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.scatter(x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.plot   (x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.scatter(x_out_mn[:,0],x_out_mn[:,1], c='g')\n","plt.plot   (x_out_mn[:,0],x_out_mn[:,1], c='g')\n","\n","\n","plt.xlim([0,300])\n","plt.ylim([100,250])\n","\n","plt.show()"],"metadata":{"id":"D53VorBS3lvn"},"id":"D53VorBS3lvn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test case 4: Rosenbrock function\n"],"metadata":{"id":"EzGqgn6qYklJ"},"id":"EzGqgn6qYklJ"},{"cell_type":"code","source":["#######################################\n","# Test case 4\n","#######################################\n","\n","# Set all the parameters\n","a = 0; b = 100;\n","\n","from sympy import symbols\n","from sympy import lambdify\n","from sympy import Matrix\n","# Define optimization variables\n","x, y = symbols('x y')\n","\n","# Define the objective to minimize\n","rosen = (a-x)**2+b*(y-x**2)**2\n","dRosen = Matrix([rosen.diff(x) for x in [x,y]]) \n","ddRosen = Matrix([[y.diff(x) for y in dRosen] for x in [x,y]]) # Calculate hessian\n","\n","\n","# lambdify the objective function and the derivative\n","rosenF    = lambdify([x,y], rosen)\n","dRosenF   = lambdify([x,y], dRosen)\n","ddRosenF  = lambdify([x,y], ddRosen)\n","\n","# define wrapper functions\n","def rosenFW (x):\n","  return rosenF(x[0],x[1])\n","\n","def dRosenFW (x):\n","  return dRosenF(x[0],x[1])\n","\n","def ddRosenFW (x):\n","  return ddRosenF(x[0],x[1])\n","\n","# execution of your optimization code \n","x0 = np.array([-25., 300.])\n","x_out_sd = sd(rosenFW,x0,dRosenFW,0.0000001)\n","print('Steepest descent gives: ' + str(x_out_sd[-1]))\n","x_out_cg = cg(rosenFW,x0,dRosenFW,0.0000001)\n","print('Conjugate gradient gives: ' + str(x_out_cg[-1]))\n","x_out_mn = mn(rosenFW,x0,dRosenFW,ddRosenFW,0.0000001)\n","print('Modified Newton gives: ' + str(x_out_mn[-1]))\n","\n","# Check your code against scipy optimize\n","from scipy.optimize import minimize\n","res = minimize(testFun, x0)\n","print('Scipy gives: ' + str(res.x))"],"metadata":{"id":"TUObxfAiYuX4"},"id":"TUObxfAiYuX4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################################\n","# Test case 4\n","#######################################\n","\n","import matplotlib.pyplot as plt\n","\n","# calculate for n data points \n","n = 10\n","\n","val_x0 = np.linspace(-20., 20., n)\n","val_x1 = np.linspace(-200., 400., n)\n","\n","# meshgrid allows us to weave these two variables to arrive at n^2 points for calculation\n","optVar_x0, optVar_x1 = np.meshgrid(val_x0, val_x1)\n","\n","# initialize a figure container\n","fig = plt.figure(figsize=(8, 6))\n","\n","# plot contour to display objective landscape\n","plt.contour(optVar_x0, optVar_x1, rosenFW([optVar_x0, optVar_x1]))\n","\n","# plot the iteration x_star values\n","plt.scatter(x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.plot   (x_out_sd[:,0],x_out_sd[:,1], c='b')\n","plt.scatter(x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.plot   (x_out_cg[:,0],x_out_cg[:,1], c='r')\n","plt.scatter(x_out_mn[:,0],x_out_mn[:,1], c='g')\n","plt.plot   (x_out_mn[:,0],x_out_mn[:,1], c='g')\n","\n","plt.show()"],"metadata":{"id":"iedQEM7CYwxN"},"id":"iedQEM7CYwxN","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"psZ9rSj-ZVNq"},"id":"psZ9rSj-ZVNq","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"colab":{"name":"assignment_02_template.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}